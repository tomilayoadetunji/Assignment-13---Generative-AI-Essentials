This project demonstrates a simple generative text model built using TensorFlow/Keras and trained on the public-domain text Alice in Wonderland. The dataset was downloaded from Project Gutenberg and processed through tokenization, sequence creation, and one-hot encoding. A word-level LSTM network was then trained on short input sequences to predict the next word, enabling basic text generation.

The model consists of an embedding layer, a single LSTM layer, and a softmax output layer over the full vocabulary. After training for 10 epochs, the model was able to generate short, coherent text continuations from a seed prompt. A small content-creation demo was included to show how the model can be applied to produce creative narrative introductions. While this LSTM approach is simpler than modern GPT-style transformers, it provides a practical foundation for understanding how generative models learn patterns and produce new text.
